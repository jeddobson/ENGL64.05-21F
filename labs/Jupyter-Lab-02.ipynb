{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 / QSS 30.16 21F</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 2\n",
    "## Punctuation, Part of Speech Tagging, Named-Entity Recognition, Segmentation, and Vectorization\n",
    "\n",
    " <center><pre>Created: 10/09/2019; Revised 09/27/2021</pre></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part One: Part of Speech Tagging</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by loading up some important libraries/packages\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "# allow for displaying of graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's learn about NLTK's Part of Speech (POS) Tagger. \n",
    "# Write a sample sentence here...\n",
    "\n",
    "test_sentence = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tokenize a sentence in order to tag the words.\n",
    "test_sentence_tokens = word_tokenize(test_sentence)\n",
    "\n",
    "# Now we run the tagger:\n",
    "nltk.pos_tag(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete list of tag types appears at the bottom of this notebook\n",
    "\n",
    "# Now let's return to the second cell and write some other kinds of sentences.\n",
    "# Experiment with words that could be nouns or verbs depending on context.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Two: Named Entity Recognition</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the Named Entities that NLTK can recognize:\n",
    "\n",
    "|NER|Example|\n",
    "|------------|-----------|\n",
    "|ORGANIZATION|Georgia-Pacific Corp., WHO|\n",
    "|PERSON|Eddy Bonte, President Obama|\n",
    "|LOCATION|Murray River, Mount Everest|\n",
    "|DATE|June, 2008-06-29|\n",
    "|TIME|two fifty a m, 1:30 p.m.|\n",
    "|MONEY|175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT|twenty pct, 18.75 %|\n",
    "|FACILITY|Washington Monument, Stonehenge|\n",
    "|GPE|South East Asia, Midlothian|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 150 English-language novels in Andrew Piper's Novel450 dataset:\n",
    "for document in sorted(glob.glob(\"shared/ENGL64.05-21F/data/Novel450/EN*\")):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of these and read it into the variable raw_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's determine how long it is (word count) using our old friend, the word_tokenizer\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 300 words (roughly a page)\n",
    "sample_tokens = tokens[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the 'Named Entity Chunker' ne_chunk to 'chunk' our tagged \n",
    "# tokens and then apply named entity recongition.\n",
    "ner_data = ne_chunk(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_type = \"PERSON\" # define NER category of interest\n",
    "\n",
    "# we'll make a dictonary to store found Named Entities\n",
    "found_objects = dict()\n",
    "\n",
    "# Run GPE \n",
    "for i in ner_data.subtrees():\n",
    "    if i.label() == ner_type: \n",
    "            ner_object = i[0][0]\n",
    "            if ner_object in found_objects:\n",
    "                found_objects[ner_object] += 1\n",
    "            else:\n",
    "                found_objects[ner_object] = 1\n",
    "\n",
    "top_objects = sorted(found_objects, key=found_objects.get, reverse=True)\n",
    "for i in top_objects:\n",
    "    print(i,found_objects[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go back and select a different range (different number of pages) of your text. \n",
    "# Then try another text.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Three: Document Segmentation</font></h3>\n",
    "\n",
    "As we just saw, it will be sometimes better to operate a small section of text. We can call these units \"segments\" and produce them automatically. With a standarized set of segments we can better understand changes throughout narrative time (the \"syuzhet\" or emplotted narrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of the above texts and (re)read it into the variable raw_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically we predetermine the number of segments we want created.\n",
    "ns = 100 # how many segments do we want to create?\n",
    "segment_length = int(len(tokens) / ns) # how many words go in each segment?\n",
    "segments = list()\n",
    "for j in range(ns):\n",
    "    seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "    segments.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin with tagging the first bucket\n",
    "pos_data = nltk.pos_tag(segments[0])\n",
    "\n",
    "# find all the proper nouns (NNP)\n",
    "found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "print(len(set(found_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display them\n",
    "found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our percent of proper nouns per bucket?\n",
    "data_to_plot=list()\n",
    "\n",
    "for s in segments:\n",
    "    total_tokens = len(s)\n",
    "    \n",
    "    # extract Part of Speech data \n",
    "    pos_data = nltk.pos_tag(s)\n",
    "    \n",
    "    # select only objects of interest\n",
    "    found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "\n",
    "    # add to list\n",
    "    data_to_plot.append((round(len(found_words)/total_tokens * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display these percentages over narrative time\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(data_to_plot))\n",
    "plt.plot(x, data_to_plot)\n",
    "plt.title(\"Distribution of Proper Nouns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this? What can this distribution of the percentage of\n",
    "# proper nouns tell us?\n",
    "\n",
    "# Now go back and change to find foreign words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Four: Punctuation</font></h3>\n",
    "\n",
    "Let's now compare the use of punctuation in two different authors.\n",
    "\n",
    "1. Select one text from two different authors\n",
    "2. Read and tokenize file. \n",
    "3. Use punct_count function to obtain dictionary of counts for 1,000 word segments\n",
    "4. Compare use of punctuation marks as mean value of instances in 1,000 word segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [\".\",\",\",\";\",\":\",\"?\",\"!\",\"â€”\",\"-\",\"[\",\"(\",\"&\",\"/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_count(tokens):\n",
    "    # create segments of 1,000 tokens\n",
    "    segment_length = 1000\n",
    "    ns = int(len(tokens) / segment_length) # how many segments are needed?\n",
    "    segments = list()\n",
    "    for j in range(ns):\n",
    "        seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        segments.append(seg)\n",
    "    punct_dict = dict()\n",
    "\n",
    "    # process each segment and count appearance of punctuation marks\n",
    "    for seg in segments:\n",
    "        for p in punctuation_list:\n",
    "            if p not in punct_dict:\n",
    "                punct_dict[p] = [seg.count(p)]\n",
    "            else:\n",
    "                punct_dict[p].append(seg.count(p))\n",
    "    return punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Five: Vectorization</font></h3>\n",
    "\n",
    "Now we're going to convert our texts into a document-term matrix. We'll use Scikit-Learn to vectorize the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename',\n",
    "                             lowercase=True,\n",
    "                             strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize all 150 English-language Novels\n",
    "input_files = glob.glob(\"shared/ENGL64.05-21F/data/Novel450/EN*\")\n",
    "\n",
    "# This does the actual vectorization\n",
    "counts = vectorizer.fit_transform(input_files)\n",
    "\n",
    "# Return total number of documents and the number of items in the vocabulary\n",
    "dc, vc = counts.shape\n",
    "print(\"document count:\",dc,\"vocabulary count:\",vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our top terms?\n",
    "vocab_sums = counts.sum(axis=0)\n",
    "sorted_vocab = [(v, vocab_sums[0, i]) for v, i in vectorizer.vocabulary_.items()]\n",
    "sorted_vocab = sorted(sorted_vocab, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display top twenty words\n",
    "for i in range(1,20):\n",
    "    print(sorted_vocab[i][0],\"->\",sorted_vocab[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now to going to limit the vocabulary.\n",
    "# Review the documentation for the vectorizer by executing this cell and modify the above line in \n",
    "# which we initialize the vectorizer from CountVectorizer. \n",
    "#\n",
    "# FIRST:\n",
    "# Remove the English language \"stopwords\" and check the top terms. What was removed? What remains?\n",
    "#\n",
    "# THEN\n",
    "# Limit the vocabulary to only those terms appearing in 75% of the documents\n",
    "\n",
    "help(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "----\n",
    "\n",
    "|Tag|Meaning|\n",
    "|---|-------|\n",
    "|CC|coordinating conjunction|\n",
    "|CD|cardinal digit|\n",
    "|DT|determiner|\n",
    "|EX|existential there|\n",
    "|FW|foreign word|\n",
    "|IN|preposition/subordinating conjunction|\n",
    "|JJ|adjective|\n",
    "|JJR|adjective, comparative|\n",
    "|JJS|adjective, superlative|\n",
    "|LS|list marker|\n",
    "|MD|modal|\n",
    "|NN|noun, singular|\n",
    "|NNS|noun plural|\n",
    "|NNP|proper noun, singular|\n",
    "|NNPS|proper noun, plural|\n",
    "|PDT|predeterminer|\n",
    "|POS|possessive ending|\n",
    "|PRP|personal pronoun|\n",
    "|PRP$|possessive pronoun|\n",
    "|RB|adverb|\n",
    "|RBR|adverb, comparative|\n",
    "|RBS|adverb, superlative|\n",
    "|RP|particle|\n",
    "|TO|to go|\n",
    "|UH|interjection|\n",
    "|VB|verb, base form|\n",
    "|VBD|verb, past tense|\n",
    "|VBG|verb, gerund/present participle|\n",
    "|VBN|verb, past participle|\n",
    "|VBP|verb, sing. present|\n",
    "|VBZ|verb, 3rd person sing. present|\n",
    "|WDT|wh-determiner which|\n",
    "|WP|wh-pronoun who, what|\n",
    "|WP\\$|possessive pronoun|\n",
    "|WRB|wh-abverb where, when|\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
