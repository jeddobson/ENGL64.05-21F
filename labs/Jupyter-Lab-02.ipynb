{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 / QSS 30.16 21F</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 2\n",
    "## Punctuation, Part of Speech Tagging, Named-Entity Recognition, Segmentation, and Vectorization\n",
    "\n",
    " <center><pre>Created: 10/09/2019; Revised 09/27/2021</pre></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part One: Part of Speech Tagging</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by loading up some important libraries/packages\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "# allow for displaying of graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's learn about NLTK's Part of Speech (POS) Tagger. \n",
    "# Write a sample sentence here...\n",
    "\n",
    "test_sentence = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tokenize a sentence in order to tag the words.\n",
    "test_sentence_tokens = word_tokenize(test_sentence)\n",
    "\n",
    "# Now we run the tagger:\n",
    "nltk.pos_tag(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete list of tag types appears at the bottom of this notebook\n",
    "\n",
    "# Now let's return to the second cell and write some other kinds of sentences.\n",
    "# Experiment with words that could be nouns or verbs depending on context.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Two: Named Entity Recognition</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the Named Entities that NLTK can recognize:\n",
    "\n",
    "|NER|Example|\n",
    "|------------|-----------|\n",
    "|ORGANIZATION|Georgia-Pacific Corp., WHO|\n",
    "|PERSON|Eddy Bonte, President Obama|\n",
    "|LOCATION|Murray River, Mount Everest|\n",
    "|DATE|June, 2008-06-29|\n",
    "|TIME|two fifty a m, 1:30 p.m.|\n",
    "|MONEY|175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT|twenty pct, 18.75 %|\n",
    "|FACILITY|Washington Monument, Stonehenge|\n",
    "|GPE|South East Asia, Midlothian|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared/ENGL64.05-21F/data/Novel450/EN_1771_Mackenzie,Henry_TheManofFeeling_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1771_Smollett,Tobias_TheExpedictionofHenryClinker_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1778_Burney,Fanny_Evelina_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1782_Burney,Fanny_Cecilia_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1786_Beckford,William_Vathek_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1788_Wollstonecraft,Mary_Mary_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1790_Radcliffe,Ann_ASicilianRomance_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1794_Godwin,William_CalebWilliams_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1794_Radcliffe,Ann_TheMysteriesofUdolpho_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1794_Rowson,Susanna_CharlotteTemple_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1795_Lewis,Matthew_TheMonk_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1796_Bonhote,Elizabeth_BungayCastle_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1796_Burney,Fanny_Camilla_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1796_Hays,Mary_EmmaCourtney_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1797_Foster,HannahWebster_TheCoquette_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1798_Brown,CharlesBrockden_Wieland_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1798_Wollstonecraft,Mary_Maria_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1799_Brown,CharlesBrockden_ArthurMervyn_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1800_Edgeworth,Maria_CastleRackrent_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1801_Edgeworth,Maria_Belinda_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1804_Opie,Amelia_AdelineMowbray_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1805_Lewis,Matthew_TheBravoofVenice_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1806_Edgeworth,Maria_Leonora_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1809_More,Hannah_CoelebsinSearchofaWife_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1811_Austen,Jane_SenseandSensibility_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1813_Austen,Jane_PrideandPrejudice_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1814_Austen,Jane_MansfieldPark_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1814_Scott,Walter_Waverley_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1815_Peacock,ThomasLove_HeadlongHall_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1817_Scott,Walter_RobRoy_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1818_Peacock,ThomasLove_NightmareAbbey_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1818_Shelley,Mary_Frankenstein_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1819_Shelley,Mary_Mathilda_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1820_Scott,Walter_Ivanhoe_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1821_Galt,John_AnnalsoftheParish_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1821_Peacock,ThomasLove_MaidMarian_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1822_Hogg,James_ThreePerilsofMan_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1823_Cooper,JamesFenimore_ThePioneers_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1826_Cooper,JameFenimore_TheLastoftheMohicans_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1826_Disraeli,Benjamin_VivianGrey_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1836_Child,Lydia_Philothea_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1837_Disraeli,Benjamin_Venetia_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1837_Trollope,FrancesMilton_TheVicarofWrexham_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1838_Martineau,Harriet_Deerbrook_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1838_Poe,EdgarAllen_TheNarrativeofArthurGordonPym_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1843_Borrow,George_TheBibleinSpain_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1844_Yonge,Charlotte_Abbeychurch_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1847_Aguilar,Grace_HomeInfluence_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1847_Bronte,Charlotte_JaneEyre_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1847_Bronte,Emily_WutheringHeights_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1847_Thackeray,William_VanityFair_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1848_Bronte,Ann_TheTenantofWildfellHall_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1848_Gaskell,Elizabeth_MaryBarton_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1849_Kingsley,Charles_AltonLocke_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1850_Aguilar,Grace_ValeofCedars_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1850_Yonge,Charlotte_Henrietta'sWish_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1851_Hawthorne,Nathaniel_TheHouseoftheSevenGables_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1851_Melville,Hermann_MobyDick_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1852_Collins,Wilkie_Basil_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1853_Craik,Dinah_Agatha'sHusband_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1853_Kingsley,Charles_Hypatia_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1853_Stowe,HarrietBeecher_UncleTom'sCabin_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1853_Yonge,Charlotte_TheHeirofRedcliffe_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1854_Gaskell,Elizabeth_NorthandSouth_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1855_Trollope,FrancesMilton_TheWidowBarnaby_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1856_Craik,Dinah_JohnHalifax_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1857_Trollope,Anthony_BarchesterTowers_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1859_Dickens,Charles_ATaleofTwoCities_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1860_Collins,Wilkie_TheWomaninWhite_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1860_Eliot,George_TheMillontheFloss_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1861_Dickens,Charles_GreatExpectations_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1862_Braddon,Mary_LadyAudley'sSecret_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1862_Eliot,George_Romola_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1864_Braddon,Mary_HenryDunbar_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1865_Carroll,Lewis_Alice'sAdventureinWonderland_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1869_Alcott,Louisa_LittleWomen_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1869_Blackmore,R.D._LornaDoone_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1869_Trollope,Anthony_PhineasFinn_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1871_Carroll,Lewis_ThroughtheLookingGlass.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1874_Hardy,Thomas_FarFromtheMaddingCrowd_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1876_Trollope,FrancesEleanor_ACharmingFellow_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1876_Twain,Mark_TheAdventuresofTomSawyer_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1877_Sewell,Anna_BlackBeauty_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1881_James,Henry_PortraitofaLady_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1882_Stevenson,RobertLouis_TreasureIsland_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1883_Braddon,Mary_TheGoldenCalf_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1884_Lyall,Edna_WeTwo_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1884_Twain,Mark_TheAdventuresofHuckleberryFinn_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1885_Barr,Amelia_JanVeeder'sWife_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1886_Stevenson,RobertLouis_JekyllandHyde_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1887_Bellamy,Edward_LookingBackward_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1888_Trollope,FrancesEleanor_ThatUnfortunateMarriage_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1888_Ward,Mrs.Humphry_RobertElsmere_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1889_Doyle,ArthurConan_TheMysteryoftheCloomber_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1890_Broughton,Rhoda_Alas!_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1890_Chopin,Kate_AtFault_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1890_Wilde,Oscar_ThePictureofDorianGray_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1891_Doyle,ArthurConan_TheDoingsofRafflesHaw_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1891_Gissing,George_NewGrubStreet_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1891_Hardy,Thomas_TessoftheD'Urbervilles_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1893_Gissing,George_TheOddWomen_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1893_Grand,Sarah_TheHeavenlyTwins_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1893_Harraden,Beatrice_ShipsThatPassintheNight_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1894_Freeman,MaryWilkins_Pembroke_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1894_Hope,Anthony_ThePrisonerofZenda_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1894_Kipling,Rudyard_TheJungleBook_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1895_Crane,Stephen_TheRedBadgeofCourage_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1895_Wells,H.G._TheTimeMachine_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1897_Stoker,Bram_Dracula_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1898_Crockett,SR_TheRedAxe_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1899_Chopin,Kate_TheAwakening_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1899_Conrad,Joseph_HeartofDarkness_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1900_Barr,Amelia_TheMaidofMaidenLane_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1900_Dreiser,Theodore_SisterCarrie_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1900_Kipling,Rudyard_Kim_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1901_Norris,Frank_TheOctopus_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1902_Bellamy,Edward_Eleonora_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1902_Bennett,Arnold_GrandBabylonHotel_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1903_James,Henry_TheAmbassadors_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1903_London,Jack_TheCalloftheWild_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1903_Norris,Frank_ThePit_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1904_Murfree,MaryNoailles_TheFrontiersman_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1905_Orczy,Emma_TheScarletPimpernel_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1905_Wharton,Edith_TheHouseofMirth_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1906_London,Jack_WhiteFang_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1906_Sinclair,Upton_TheJungle_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1906_Stein,Gertrude_ThreeLives_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1908_Forster,E.M._ARoomWithaView_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1910_Forster,E.M._HowardsEnd_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1911_Barrie,J.M._PeterPan_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1911_Wharton,Edith_EthanFrome_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1912_Cather,Willa_Alexander'sBridge_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1912_Dreiser,Theodore_TheFinancier_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1913_Lawrence,D.H._SonsandLovers_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1915_Ford,FordMadox_TheGoodSoldier_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1916_Joyce,James_APortraitoftheArtistasaYoungMan_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1917_Cahan,Abraham_TheRiseofDavidLevinsky_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1917_Lewis,Sinclair_TheInnocents_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1917_Webb,Mary_GonetoEart_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1918_Lewis,Sinclai_TheJob_Nove.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1920_DosPassos,John_ThreeSoldiers_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1920_Wharton,Edith_TheAgeofInnocence_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1922_Fitzgerald,FScott_TheBeautifulandtheDamned_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1922_Joyce,James_Ulysses_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1927_Woolf,Virginia_TotheLighthouse_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1928_Woolf,Virginia_Orlando_Novel.txt\n",
      "shared/ENGL64.05-21F/data/Novel450/EN_1930_Mansfield,Katherine_TheAloe_Novel.txt\n"
     ]
    }
   ],
   "source": [
    "# There are 150 English-language novels in Andrew Piper's Novel450 dataset:\n",
    "for document in sorted(glob.glob(\"shared/ENGL64.05-21F/data/Novel450/EN*\")):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of these and read it into the variable raw_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's determine how long it is (word count) using our old friend, the word_tokenizer\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 300 words (roughly a page)\n",
    "sample_tokens = tokens[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the 'Named Entity Chunker' ne_chunk to 'chunk' our tagged \n",
    "# tokens and then apply named entity recongition.\n",
    "ner_data = ne_chunk(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_type = \"PERSON\" # define NER category of interest\n",
    "\n",
    "# we'll make a dictonary to store found Named Entities\n",
    "found_objects = dict()\n",
    "\n",
    "# Run GPE \n",
    "for i in ner_data.subtrees():\n",
    "    if i.label() == ner_type: \n",
    "            ner_object = i[0][0]\n",
    "            if ner_object in found_objects:\n",
    "                found_objects[ner_object] += 1\n",
    "            else:\n",
    "                found_objects[ner_object] = 1\n",
    "\n",
    "top_objects = sorted(found_objects, key=found_objects.get, reverse=True)\n",
    "for i in top_objects:\n",
    "    print(i,found_objects[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go back and select a different range (different number of pages) of your text. \n",
    "# Then try another text.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Three: Document Segmentation</font></h3>\n",
    "\n",
    "As we just saw, it will be sometimes better to operate a small section of text. We can call these units \"segments\" and produce them automatically. With a standarized set of segments we can better understand changes throughout narrative time (the \"syuzhet\" or emplotted narrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of the above texts and (re)read it into the variable raw_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically we predetermine the number of segments we want created.\n",
    "ns = 100 # how many segments do we want to create?\n",
    "segment_length = int(len(tokens) / ns) # how many words go in each segment?\n",
    "segments = list()\n",
    "for j in range(ns):\n",
    "    seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "    segments.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin with tagging the first bucket\n",
    "pos_data = nltk.pos_tag(segments[0])\n",
    "\n",
    "# find all the proper nouns (NNP)\n",
    "found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "print(len(set(found_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display them\n",
    "found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our percent of proper nouns per bucket?\n",
    "data_to_plot=list()\n",
    "\n",
    "for s in segments:\n",
    "    total_tokens = len(s)\n",
    "    \n",
    "    # extract Part of Speech data \n",
    "    pos_data = nltk.pos_tag(s)\n",
    "    \n",
    "    # select only objects of interest\n",
    "    found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "\n",
    "    # add to list\n",
    "    data_to_plot.append((round(len(found_words)/total_tokens * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display these percentages over narrative time\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(data_to_plot))\n",
    "plt.plot(x, data_to_plot)\n",
    "plt.title(\"Distribution of Proper Nouns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this? What can this distribution of the percentage of\n",
    "# proper nouns tell us?\n",
    "\n",
    "# Now go back and change to find foreign words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Four: Punctuation</font></h3>\n",
    "\n",
    "Let's now compare the use of punctuation in two different authors.\n",
    "\n",
    "1. Select one text from two different authors\n",
    "2. Read and tokenize file. \n",
    "3. Use punct_count function to obtain dictionary of counts for 1,000 word segments\n",
    "4. Compare use of punctuation marks as mean value of instances in 1,000 word segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 87642 tokens\n"
     ]
    }
   ],
   "source": [
    "punctuation_list = [\".\",\",\",\";\",\":\",\"?\",\"!\",\"â€”\",\"-\",\"[\",\"(\",\"&\",\"/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_count(tokens):\n",
    "    # create segments of 1,000 tokens\n",
    "    segment_length = 1000\n",
    "    ns = int(len(tokens) / segment_length) # how many segments are needed?\n",
    "    segments = list()\n",
    "    for j in range(ns):\n",
    "        seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        segments.append(seg)\n",
    "    punct_dict = dict()\n",
    "\n",
    "    # process each segment and count appearance of punctuation marks\n",
    "    for seg in segments:\n",
    "        for p in punctuation_list:\n",
    "            if p not in punct_dict:\n",
    "                punct_dict[p] = [seg.count(p)]\n",
    "            else:\n",
    "                punct_dict[p].append(seg.count(p))\n",
    "    return punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part Five: Vectorization</font></h3>\n",
    "\n",
    "Now we're going to convert our texts into a document-term matrix. We'll use Scikit-Learn to vectorize the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename',\n",
    "                             lowercase=True,\n",
    "                             strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document count: 150 vocabulary count: 99050\n"
     ]
    }
   ],
   "source": [
    "# Vectorize all 150 English-language Novels\n",
    "input_files = glob.glob(\"shared/ENGL64.05-21F/data/Novel450/EN*\")\n",
    "\n",
    "# This does the actual vectorization\n",
    "counts = vectorizer.fit_transform(input_files)\n",
    "\n",
    "# Return total number of documents and the number of items in the vocabulary\n",
    "dc, vc = counts.shape\n",
    "print(\"document count:\",dc,\"vocabulary count:\",vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and -> 600098\n",
      "to -> 540505\n",
      "of -> 537478\n",
      "in -> 292357\n",
      "he -> 254922\n",
      "was -> 252419\n",
      "that -> 242275\n",
      "her -> 222367\n",
      "it -> 220541\n",
      "his -> 196580\n",
      "you -> 189068\n",
      "she -> 182289\n",
      "had -> 166038\n",
      "with -> 164062\n",
      "as -> 154436\n",
      "for -> 151373\n",
      "not -> 134129\n",
      "but -> 128827\n",
      "at -> 119437\n"
     ]
    }
   ],
   "source": [
    "# what are our top terms?\n",
    "vocab_sums = counts.sum(axis=0)\n",
    "sorted_vocab = [(v, vocab_sums[0, i]) for v, i in vectorizer.vocabulary_.items()]\n",
    "sorted_vocab = sorted(sorted_vocab, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display top twenty words\n",
    "for i in range(1,20):\n",
    "    print(sorted_vocab[i][0],\"->\",sorted_vocab[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}, default='content'\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We're now to going to limit the vocabulary.\n",
    "# Review the documentation for the vectorizer by executing this cell and modify the above line in \n",
    "# which we initialize the vectorizer from CountVectorizer. \n",
    "#\n",
    "# FIRST:\n",
    "# Remove the English language \"stopwords\" and check the top terms. What was removed? What remains?\n",
    "#\n",
    "# THEN\n",
    "# Limit the vocabulary to only those terms appearing in 75% of the documents\n",
    "\n",
    "help(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "----\n",
    "\n",
    "|Tag|Meaning|\n",
    "|---|-------|\n",
    "|CC|coordinating conjunction|\n",
    "|CD|cardinal digit|\n",
    "|DT|determiner|\n",
    "|EX|existential there|\n",
    "|FW|foreign word|\n",
    "|IN|preposition/subordinating conjunction|\n",
    "|JJ|adjective|\n",
    "|JJR|adjective, comparative|\n",
    "|JJS|adjective, superlative|\n",
    "|LS|list marker|\n",
    "|MD|modal|\n",
    "|NN|noun, singular|\n",
    "|NNS|noun plural|\n",
    "|NNP|proper noun, singular|\n",
    "|NNPS|proper noun, plural|\n",
    "|PDT|predeterminer|\n",
    "|POS|possessive ending|\n",
    "|PRP|personal pronoun|\n",
    "|PRP$|possessive pronoun|\n",
    "|RB|adverb|\n",
    "|RBR|adverb, comparative|\n",
    "|RBS|adverb, superlative|\n",
    "|RP|particle|\n",
    "|TO|to go|\n",
    "|UH|interjection|\n",
    "|VB|verb, base form|\n",
    "|VBD|verb, past tense|\n",
    "|VBG|verb, gerund/present participle|\n",
    "|VBN|verb, past participle|\n",
    "|VBP|verb, sing. present|\n",
    "|VBZ|verb, 3rd person sing. present|\n",
    "|WDT|wh-determiner which|\n",
    "|WP|wh-pronoun who, what|\n",
    "|WP\\$|possessive pronoun|\n",
    "|WRB|wh-abverb where, when|\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
